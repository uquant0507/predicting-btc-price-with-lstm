{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb-1d ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaFDVBI5uLaUEmD82JqcfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/uquant0507/f9779ba50032cc6a850ccc326e2eb0aa/lstm-ipynb-1d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Python libraries"
      ],
      "metadata": {
        "id": "O3XQb4p3bAoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install numpy, pandas, pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "# pip install torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable \n",
        "import torch.nn.init as init\n",
        "\n",
        "# pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# pip install ccxt\n",
        "!pip install ccxt\n",
        "import ccxt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NkmbBSsbr8m",
        "outputId": "3abdc0e0-0656-47d4-d0b4-628ff5b31c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ccxt\n",
            "  Downloading ccxt-1.72.84-py2.py3-none-any.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.7/dist-packages (from ccxt) (2021.10.8)\n",
            "Collecting yarl==1.7.2\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 11.6 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.6.1\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 34.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=38.5.1 in /usr/local/lib/python3.7/dist-packages (from ccxt) (57.4.0)\n",
            "Collecting aiohttp>=3.8\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.2 MB/s \n",
            "\u001b[?25hCollecting aiodns>=1.1.1\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from ccxt) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt) (3.10.0.2)\n",
            "Collecting multidict>=4.0\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt) (2.10)\n",
            "Collecting pycares>=4.0.0\n",
            "  Downloading pycares-4.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt) (2.0.11)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.6.1->ccxt) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->ccxt) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->ccxt) (3.0.4)\n",
            "Installing collected packages: multidict, frozenlist, yarl, pycares, asynctest, async-timeout, aiosignal, cryptography, aiohttp, aiodns, ccxt\n",
            "Successfully installed aiodns-3.0.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 ccxt-1.72.84 cryptography-36.0.1 frozenlist-1.3.0 multidict-6.0.2 pycares-4.1.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"data\": {\n",
        "        \"window_size\": 14,\n",
        "        \"train_split_size\": 0.5,\n",
        "        \"val_split_size\": 0.2,\n",
        "        \"thres_frac\": 1,\n",
        "        \"change_time\": 3\n",
        "    }, \n",
        "    \"model\": {\n",
        "        \"input_size\": 2, # price, volume\n",
        "        \"num_lstm_layers\": 1,\n",
        "        \"hidden_size\": 32,\n",
        "        \"num_classes\" : 3,\n",
        "        \"dropout\": 0.2,\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"device\": \"cuda\", # \"cuda\" or \"cpu\"\n",
        "        \"batch_size\": 32,\n",
        "        \"epoch\": 100,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"scheduler_step_size\": 40,\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "tul_QAEsT60h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Preparation"
      ],
      "metadata": {
        "id": "WCfdy4fybvmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1)Fetch Data"
      ],
      "metadata": {
        "id": "u_LL3glyhyZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binance = ccxt.binance()\n",
        "fetch_num = 500\n",
        "timeframe = '1d'\n",
        "ticker = binance.fetch_ohlcv(\"BTC/USDT\", timeframe, limit=fetch_num)\n",
        "startfrom = ticker[0][0]\n",
        "class ohlcv:\n",
        "  def __init__(self, index):\n",
        "    self.index = index\n",
        "    since = startfrom - 1800000000 * 24 * self.index\n",
        "    ohlcv = binance.fetch_ohlcv(\"BTC/USDT\", timeframe, since=since, limit=fetch_num)\n",
        "    self.df = pd.DataFrame(ohlcv, columns=['datetime', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "ohlcv_df = pd.concat([ohlcv(2).df, ohlcv(1).df, ohlcv(0).df])\n",
        "ohlcv_df = ohlcv_df.drop_duplicates(['datetime'])\n",
        "ohlcv_df = ohlcv_df.drop(ohlcv_df.loc[ohlcv_df['close']==0].index)\n",
        "ohlcv_df = ohlcv_df.drop(ohlcv_df.loc[ohlcv_df['volume']==0].index)\n",
        "ohlcv_df['p_change'] = ohlcv_df['close'] - ohlcv_df['close'].shift(1)\n",
        "ohlcv_df['p_changerate'] = np.log(ohlcv_df['close']/ ohlcv_df['close'].shift(1))\n",
        "ohlcv_df['v_changerate'] = np.log(ohlcv_df['volume']/ ohlcv_df['volume'].shift(1))\n",
        "ohlcv_df = ohlcv_df.drop([0])\n",
        "ohlcv_df['datetime'] = pd.to_datetime(ohlcv_df['datetime'], unit='ms')\n",
        "datetime = ohlcv_df['datetime'].to_numpy()\n",
        "ohlcv_df.set_index('datetime', inplace=True)\n",
        "ohlcv_df.to_excel(\"ohlcv3.xlsx\")\n",
        "print(ohlcv_df.describe())\n",
        "p_change = ohlcv_df['p_change'].to_numpy()\n",
        "p_changerate = ohlcv_df['p_changerate'].to_numpy()\n",
        "v_changerate = ohlcv_df['v_changerate'].to_numpy()\n",
        "print(p_change.shape)"
      ],
      "metadata": {
        "id": "SD2GJYpEh2Dv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1d6668-f55d-4865-e805-4b26d33204e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               open          high  ...  p_changerate  v_changerate\n",
            "count   1497.000000   1497.000000  ...   1497.000000   1497.000000\n",
            "mean   19011.240728  19558.373674  ...      0.000637     -0.000943\n",
            "std    17941.713338  18458.808690  ...      0.040917      0.396504\n",
            "min     3211.710000   3276.500000  ...     -0.502607     -3.687514\n",
            "25%     7131.590000   7340.000000  ...     -0.016147     -0.247163\n",
            "50%     9575.000000   9759.820000  ...      0.001501     -0.030184\n",
            "75%    33517.090000  34749.000000  ...      0.018675      0.242643\n",
            "max    67525.820000  69000.000000  ...      0.178449      2.599870\n",
            "\n",
            "[8 rows x 8 columns]\n",
            "(1497,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "cGiBMNxCB4pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2)Data into train, test data"
      ],
      "metadata": {
        "id": "SLIfYv0N7KF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize\n",
        "def normalize(x):\n",
        "  mu = np.average(x)\n",
        "  sd = np.std(x)\n",
        "  normalized_x = (x - mu) / sd\n",
        "  print(mu)\n",
        "  print(sd)\n",
        "  return normalized_x\n",
        "\n",
        "p_changerate = normalize(p_changerate)\n",
        "v_changerate = normalize(v_changerate)\n",
        "\n",
        "#perform windowing\n",
        "def prepare_data_x(p_changerate, v_changerate, window_size):\n",
        "   n_row = p_changerate.shape[0] - window_size + 1\n",
        "   x1 = np.lib.stride_tricks.as_strided(p_changerate, shape=(n_row,window_size), strides=(p_changerate.strides[0],p_changerate.strides[0]))\n",
        "   x2 = np.lib.stride_tricks.as_strided(v_changerate, shape=(n_row,window_size), strides=(v_changerate.strides[0],v_changerate.strides[0]))\n",
        "   li = [-i for i in range(1, config[\"data\"][\"change_time\"]+1)]\n",
        "   x1 = np.delete(x1, li, axis=0)\n",
        "   x2 = np.delete(x2, li, axis=0)\n",
        "   x = np.dstack([x1, x2])\n",
        "   x = x.astype(np.float32)\n",
        "   return x\n",
        "\n",
        "#calculate deviation by window\n",
        "def prepare_data_y(p_change, window_size, fraction):\n",
        "  n_row = p_change.shape[0] - window_size + 1\n",
        "  windowed_change = np.lib.stride_tricks.as_strided(p_change, shape=(n_row,window_size), strides=(p_change.strides[0],p_change.strides[0]))\n",
        "  threshold = fraction * np.std(windowed_change, axis=1)\n",
        "  print(threshold.shape)\n",
        "  yn = np.zeros(n_row-1)\n",
        "  for i in range(n_row-1):\n",
        "    li = p_change[window_size+i:window_size+i+ config[\"data\"][\"change_time\"]]\n",
        "    change = sum(li)\n",
        "    if change >= threshold[i]:\n",
        "      yn[i] = 1\n",
        "    elif change <= -threshold[i]:\n",
        "      yn[i] = 2\n",
        "  return yn\n",
        "\n",
        "\n",
        "x = prepare_data_x(p_changerate, v_changerate, config[\"data\"][\"window_size\"])\n",
        "y = prepare_data_y(p_change, config[\"data\"][\"window_size\"],  config[\"data\"][\"thres_frac\"])\n",
        "\n",
        "# split dataset\n",
        "def split_dataset(x, y, shuffle=False):\n",
        "  index = int(x.shape[0]* config[\"data\"][\"train_split_size\"])\n",
        "  index2 = int(x.shape[0]* (config[\"data\"][\"train_split_size\"] + config[\"data\"][\"val_split_size\"]))\n",
        "  x_train = x[:index]\n",
        "  x_val = x[index:index2]\n",
        "  x_test = x[index2:]\n",
        "  y_train = y[:index]\n",
        "  y_val = y[index:index2]\n",
        "  y_test = y[index2:]\n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "x_train = split_dataset(x, y)[0]\n",
        "y_train = split_dataset(x, y)[1]\n",
        "x_val = split_dataset(x, y)[2]\n",
        "y_val = split_dataset(x, y)[3]\n",
        "x_test = split_dataset(x, y)[4]\n",
        "y_test = split_dataset(x, y)[5]\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x.astype(np.float32)\n",
        "        self.y = y.astype(np.int64)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.x[idx], self.y[idx])\n",
        "\n",
        "dataset_train = TimeSeriesDataset(x_train, y_train)\n",
        "dataset_val = TimeSeriesDataset(x_val, y_val)\n",
        "dataset_test = TimeSeriesDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "val_loader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "test_loader = DataLoader(dataset_test, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "\n",
        "for (x_train, y_train) in train_loader: \n",
        "  print(x_train.size(), x_train.type())\n",
        "  print(y_train.size(), y_train.type())\n",
        "  break"
      ],
      "metadata": {
        "id": "NGNAHCnv7Xxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973cef7f-efc6-4430-83f1-4f2a41fd0ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0006372810644488457\n",
            "0.040903813092885676\n",
            "-0.0009429305394568896\n",
            "0.3963716782793949\n",
            "(1484,)\n",
            "torch.Size([32, 14, 2]) torch.FloatTensor\n",
            "torch.Size([32]) torch.LongTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Defining Model"
      ],
      "metadata": {
        "id": "vyFETWkdIkeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes #number of classes\n",
        "    self.num_layers = num_layers #number of layers\n",
        "    self.input_size = input_size #input size\n",
        "    self.hidden_size = hidden_size #hidden state\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm1\n",
        "    self.fc1 = nn.Linear(hidden_size, 256) #fully connected 1\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, num_classes) #fully connected last layer\n",
        "    self.relu = nn.ReLU()\n",
        "    self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(256)\n",
        "    self.batch_norm3 = nn.BatchNorm1d(128)\n",
        "    self.dropout_prob = 0.2\n",
        "  \n",
        "  def forward(self,x):\n",
        "    # Propagate input through LSTM\n",
        "    output, (hn, cn) = self.lstm(x) #lstm with input, hidden, and internal state\n",
        "    x = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "    x = self.batch_norm1(x)\n",
        "    x = self.fc1(x) #first Dense\n",
        "    x = self.batch_norm2(x)\n",
        "    x = self.relu(x) #relu\n",
        "    x = F.dropout(x, training=self.training, p=self.dropout_prob)\n",
        "    x = self.fc2(x) #first Dense\n",
        "    x = self.batch_norm3(x)\n",
        "    x = self.relu(x) #relu\n",
        "    x = F.dropout(x, training=self.training, p=self.dropout_prob)\n",
        "    x = self.fc3(x) #Final Output\n",
        "    x = F.log_softmax(x)\n",
        "    return x\n",
        "\n",
        "model = LSTMModel(num_classes=config[\"model\"][\"num_classes\"], input_size=config[\"model\"][\"input_size\"], hidden_size=config[\"model\"][\"hidden_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"])\n",
        "model = model.to(config[\"training\"][\"device\"])"
      ],
      "metadata": {
        "id": "G7aAqzloInl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Optimizer, Objective Function 설정하기"
      ],
      "metadata": {
        "id": "4qnz2nMUpB5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def weight_init(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    nn.init.kaiming_uniform(m.weight.data)\n",
        "model.apply(weight_init)"
      ],
      "metadata": {
        "id": "bK14LBmApNgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d818fc1-0c3b-4cd6-d51e-808a5a7d1297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(2, 16, batch_first=True)\n",
              "  (fc1): Linear(in_features=16, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (batch_norm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Train model"
      ],
      "metadata": {
        "id": "i38xX6mdpk9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, log_interval = 20):\n",
        "  model.train()\n",
        "  for idx, (x, y) in enumerate(train_loader):\n",
        "    x = x.to(config[\"training\"][\"device\"])\n",
        "    y = y.to(config[\"training\"][\"device\"])\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if idx % log_interval == 0:\n",
        "      print(\"Train Epoch: {} [{}/{} ({:.0f}%)] Train Loss: {:.6f}\".format(epoch, idx * len(x),\n",
        "      len(train_loader.dataset), 100. * idx / len(train_loader), loss.item()))\n",
        "    if idx == len(train_loader) - 1:\n",
        "      return loss.item()"
      ],
      "metadata": {
        "id": "F2aO4Muapnls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Evaluate Model"
      ],
      "metadata": {
        "id": "b_-0qRXWRzHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  ups = 0 \n",
        "  downs = 0\n",
        "  zeros = 0 \n",
        "  up_true_positive = 0\n",
        "  down_true_positive = 0\n",
        "  zero_true_positive = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "      x = x.to(config[\"training\"][\"device\"])\n",
        "      y = y.to(config[\"training\"][\"device\"])\n",
        "      out = model(x)\n",
        "      test_loss += criterion(out, y).item()\n",
        "      prediction = out.max(1, True)[1]\n",
        "      label = y.view_as(prediction)\n",
        "      correct += prediction.eq(label).sum().item()\n",
        "      ups += (prediction == 1).sum().item()\n",
        "      downs += (prediction == 2).sum().item()\n",
        "      zeros += (prediction == 0).sum().item()\n",
        "      for i in range(len(prediction)):\n",
        "        if prediction[i] == 1:\n",
        "          if prediction[i] == label[i]:\n",
        "            up_true_positive += 1\n",
        "        elif prediction[i] == 2:\n",
        "          if prediction[i] == label[i]:\n",
        "            down_true_positive += 1       \n",
        "        else: \n",
        "          if prediction[i] == label[i]:\n",
        "            zero_true_positive += 1 \n",
        "\n",
        "  up_accuracy = 100 * up_true_positive / (ups + 1e-7)\n",
        "  down_accuracy = 100 * down_true_positive / (downs + 1e-7)\n",
        "  zero_accuracy = 100 * zero_true_positive / (zeros + 1e-7)\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "  return test_loss, test_accuracy, prediction, out, up_accuracy, down_accuracy, zero_accuracy"
      ],
      "metadata": {
        "id": "aGDJDH-bR3V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 학습 진행 및 평가"
      ],
      "metadata": {
        "id": "LEC1sTssUBHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloss = []\n",
        "valloss = []\n",
        "valaccuracy = []\n",
        "upaccuracy = []\n",
        "downaccuracy = []\n",
        "zeroaccuracy = []\n",
        "for epoch in range(1, config[\"training\"][\"epoch\"] + 1):\n",
        "  x = train(model, train_loader, optimizer, log_interval=100)\n",
        "  trainloss.append(x)\n",
        "  test_loss, test_accuracy, prediction, out, up_accuracy, down_accuracy, zero_accuracy = evaluate(model, val_loader)\n",
        "  valloss.append(test_loss)\n",
        "  valaccuracy.append(test_accuracy)\n",
        "  upaccuracy.append(up_accuracy)\n",
        "  downaccuracy.append(down_accuracy)\n",
        "  zeroaccuracy.append(zero_accuracy)\n",
        "  print(\"[EPOCH: {}], Test Loss: {:.4f}, Test Accuracy: {:.2f} %\".format(epoch, test_loss, test_accuracy))\n",
        "val_data = {'train loss' : trainloss, 'val loss' : valloss, 'val accuracy' : valaccuracy, 'up accuracy' : upaccuracy, 'down accuracy' : downaccuracy, 'zero accuracy' : zeroaccuracy}\n",
        "val_result = pd.DataFrame(val_data)\n",
        "val_result.to_excel(\"val_result.xlsx\")\n"
      ],
      "metadata": {
        "id": "sTpRqo8dUiDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8fcba2c-bb31-4000-937b-0bb50c2cccc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/740 (0%)] Train Loss: 1.377048\n",
            "[EPOCH: 1], Test Loss: 0.0343, Test Accuracy: 47.97 %\n",
            "Train Epoch: 2 [0/740 (0%)] Train Loss: 1.197321\n",
            "[EPOCH: 2], Test Loss: 0.0350, Test Accuracy: 50.68 %\n",
            "Train Epoch: 3 [0/740 (0%)] Train Loss: 1.300864\n",
            "[EPOCH: 3], Test Loss: 0.0361, Test Accuracy: 48.31 %\n",
            "Train Epoch: 4 [0/740 (0%)] Train Loss: 1.054554\n",
            "[EPOCH: 4], Test Loss: 0.0377, Test Accuracy: 43.92 %\n",
            "Train Epoch: 5 [0/740 (0%)] Train Loss: 0.955547\n",
            "[EPOCH: 5], Test Loss: 0.0468, Test Accuracy: 25.68 %\n",
            "Train Epoch: 6 [0/740 (0%)] Train Loss: 0.970758\n",
            "[EPOCH: 6], Test Loss: 0.0392, Test Accuracy: 36.15 %\n",
            "Train Epoch: 7 [0/740 (0%)] Train Loss: 0.841030\n",
            "[EPOCH: 7], Test Loss: 0.0353, Test Accuracy: 51.69 %\n",
            "Train Epoch: 8 [0/740 (0%)] Train Loss: 0.795482\n",
            "[EPOCH: 8], Test Loss: 0.0390, Test Accuracy: 37.84 %\n",
            "Train Epoch: 9 [0/740 (0%)] Train Loss: 0.842456\n",
            "[EPOCH: 9], Test Loss: 0.0396, Test Accuracy: 36.15 %\n",
            "Train Epoch: 10 [0/740 (0%)] Train Loss: 0.747145\n",
            "[EPOCH: 10], Test Loss: 0.0447, Test Accuracy: 31.42 %\n",
            "Train Epoch: 11 [0/740 (0%)] Train Loss: 0.604689\n",
            "[EPOCH: 11], Test Loss: 0.0573, Test Accuracy: 23.99 %\n",
            "Train Epoch: 12 [0/740 (0%)] Train Loss: 0.733440\n",
            "[EPOCH: 12], Test Loss: 0.0451, Test Accuracy: 36.82 %\n",
            "Train Epoch: 13 [0/740 (0%)] Train Loss: 0.675537\n",
            "[EPOCH: 13], Test Loss: 0.0563, Test Accuracy: 27.03 %\n",
            "Train Epoch: 14 [0/740 (0%)] Train Loss: 0.658546\n",
            "[EPOCH: 14], Test Loss: 0.0502, Test Accuracy: 44.59 %\n",
            "Train Epoch: 15 [0/740 (0%)] Train Loss: 0.706024\n",
            "[EPOCH: 15], Test Loss: 0.0444, Test Accuracy: 44.93 %\n",
            "Train Epoch: 16 [0/740 (0%)] Train Loss: 0.497135\n",
            "[EPOCH: 16], Test Loss: 0.0574, Test Accuracy: 28.38 %\n",
            "Train Epoch: 17 [0/740 (0%)] Train Loss: 0.426998\n",
            "[EPOCH: 17], Test Loss: 0.0436, Test Accuracy: 44.26 %\n",
            "Train Epoch: 18 [0/740 (0%)] Train Loss: 0.444639\n",
            "[EPOCH: 18], Test Loss: 0.0480, Test Accuracy: 36.82 %\n",
            "Train Epoch: 19 [0/740 (0%)] Train Loss: 0.526722\n",
            "[EPOCH: 19], Test Loss: 0.0646, Test Accuracy: 30.74 %\n",
            "Train Epoch: 20 [0/740 (0%)] Train Loss: 0.411541\n",
            "[EPOCH: 20], Test Loss: 0.0706, Test Accuracy: 32.43 %\n",
            "Train Epoch: 21 [0/740 (0%)] Train Loss: 0.369780\n",
            "[EPOCH: 21], Test Loss: 0.0891, Test Accuracy: 30.74 %\n",
            "Train Epoch: 22 [0/740 (0%)] Train Loss: 0.368888\n",
            "[EPOCH: 22], Test Loss: 0.0900, Test Accuracy: 27.03 %\n",
            "Train Epoch: 23 [0/740 (0%)] Train Loss: 0.441857\n",
            "[EPOCH: 23], Test Loss: 0.0504, Test Accuracy: 39.19 %\n",
            "Train Epoch: 24 [0/740 (0%)] Train Loss: 0.404709\n",
            "[EPOCH: 24], Test Loss: 0.0566, Test Accuracy: 35.81 %\n",
            "Train Epoch: 25 [0/740 (0%)] Train Loss: 0.550023\n",
            "[EPOCH: 25], Test Loss: 0.0583, Test Accuracy: 35.81 %\n",
            "Train Epoch: 26 [0/740 (0%)] Train Loss: 0.268032\n",
            "[EPOCH: 26], Test Loss: 0.0648, Test Accuracy: 34.80 %\n",
            "Train Epoch: 27 [0/740 (0%)] Train Loss: 0.333082\n",
            "[EPOCH: 27], Test Loss: 0.0807, Test Accuracy: 31.08 %\n",
            "Train Epoch: 28 [0/740 (0%)] Train Loss: 0.359390\n",
            "[EPOCH: 28], Test Loss: 0.0780, Test Accuracy: 35.14 %\n",
            "Train Epoch: 29 [0/740 (0%)] Train Loss: 0.320058\n",
            "[EPOCH: 29], Test Loss: 0.0726, Test Accuracy: 34.46 %\n",
            "Train Epoch: 30 [0/740 (0%)] Train Loss: 0.271687\n",
            "[EPOCH: 30], Test Loss: 0.0639, Test Accuracy: 40.20 %\n",
            "Train Epoch: 31 [0/740 (0%)] Train Loss: 0.244733\n",
            "[EPOCH: 31], Test Loss: 0.0897, Test Accuracy: 32.09 %\n",
            "Train Epoch: 32 [0/740 (0%)] Train Loss: 0.196632\n",
            "[EPOCH: 32], Test Loss: 0.0670, Test Accuracy: 35.14 %\n",
            "Train Epoch: 33 [0/740 (0%)] Train Loss: 0.221182\n",
            "[EPOCH: 33], Test Loss: 0.0756, Test Accuracy: 32.43 %\n",
            "Train Epoch: 34 [0/740 (0%)] Train Loss: 0.165537\n",
            "[EPOCH: 34], Test Loss: 0.0801, Test Accuracy: 38.85 %\n",
            "Train Epoch: 35 [0/740 (0%)] Train Loss: 0.176914\n",
            "[EPOCH: 35], Test Loss: 0.0973, Test Accuracy: 30.07 %\n",
            "Train Epoch: 36 [0/740 (0%)] Train Loss: 0.200571\n",
            "[EPOCH: 36], Test Loss: 0.0685, Test Accuracy: 42.57 %\n",
            "Train Epoch: 37 [0/740 (0%)] Train Loss: 0.178117\n",
            "[EPOCH: 37], Test Loss: 0.0829, Test Accuracy: 42.57 %\n",
            "Train Epoch: 38 [0/740 (0%)] Train Loss: 0.164855\n",
            "[EPOCH: 38], Test Loss: 0.0913, Test Accuracy: 32.43 %\n",
            "Train Epoch: 39 [0/740 (0%)] Train Loss: 0.124581\n",
            "[EPOCH: 39], Test Loss: 0.0843, Test Accuracy: 35.81 %\n",
            "Train Epoch: 40 [0/740 (0%)] Train Loss: 0.255338\n",
            "[EPOCH: 40], Test Loss: 0.0865, Test Accuracy: 39.53 %\n",
            "Train Epoch: 41 [0/740 (0%)] Train Loss: 0.112505\n",
            "[EPOCH: 41], Test Loss: 0.1240, Test Accuracy: 32.09 %\n",
            "Train Epoch: 42 [0/740 (0%)] Train Loss: 0.209810\n",
            "[EPOCH: 42], Test Loss: 0.1006, Test Accuracy: 40.20 %\n",
            "Train Epoch: 43 [0/740 (0%)] Train Loss: 0.275284\n",
            "[EPOCH: 43], Test Loss: 0.1045, Test Accuracy: 35.47 %\n",
            "Train Epoch: 44 [0/740 (0%)] Train Loss: 0.087497\n",
            "[EPOCH: 44], Test Loss: 0.0944, Test Accuracy: 35.81 %\n",
            "Train Epoch: 45 [0/740 (0%)] Train Loss: 0.371078\n",
            "[EPOCH: 45], Test Loss: 0.0895, Test Accuracy: 38.51 %\n",
            "Train Epoch: 46 [0/740 (0%)] Train Loss: 0.244891\n",
            "[EPOCH: 46], Test Loss: 0.1125, Test Accuracy: 34.12 %\n",
            "Train Epoch: 47 [0/740 (0%)] Train Loss: 0.210840\n",
            "[EPOCH: 47], Test Loss: 0.0998, Test Accuracy: 37.84 %\n",
            "Train Epoch: 48 [0/740 (0%)] Train Loss: 0.079940\n",
            "[EPOCH: 48], Test Loss: 0.0945, Test Accuracy: 36.15 %\n",
            "Train Epoch: 49 [0/740 (0%)] Train Loss: 0.062770\n",
            "[EPOCH: 49], Test Loss: 0.0935, Test Accuracy: 41.89 %\n",
            "Train Epoch: 50 [0/740 (0%)] Train Loss: 0.260271\n",
            "[EPOCH: 50], Test Loss: 0.1407, Test Accuracy: 32.09 %\n",
            "Train Epoch: 51 [0/740 (0%)] Train Loss: 0.115708\n",
            "[EPOCH: 51], Test Loss: 0.1251, Test Accuracy: 37.16 %\n",
            "Train Epoch: 52 [0/740 (0%)] Train Loss: 0.094827\n",
            "[EPOCH: 52], Test Loss: 0.1043, Test Accuracy: 37.50 %\n",
            "Train Epoch: 53 [0/740 (0%)] Train Loss: 0.118865\n",
            "[EPOCH: 53], Test Loss: 0.1157, Test Accuracy: 31.76 %\n",
            "Train Epoch: 54 [0/740 (0%)] Train Loss: 0.153940\n",
            "[EPOCH: 54], Test Loss: 0.1096, Test Accuracy: 35.14 %\n",
            "Train Epoch: 55 [0/740 (0%)] Train Loss: 0.061510\n",
            "[EPOCH: 55], Test Loss: 0.1242, Test Accuracy: 35.81 %\n",
            "Train Epoch: 56 [0/740 (0%)] Train Loss: 0.037000\n",
            "[EPOCH: 56], Test Loss: 0.1134, Test Accuracy: 36.49 %\n",
            "Train Epoch: 57 [0/740 (0%)] Train Loss: 0.014043\n",
            "[EPOCH: 57], Test Loss: 0.1154, Test Accuracy: 36.82 %\n",
            "Train Epoch: 58 [0/740 (0%)] Train Loss: 0.032378\n",
            "[EPOCH: 58], Test Loss: 0.1225, Test Accuracy: 35.81 %\n",
            "Train Epoch: 59 [0/740 (0%)] Train Loss: 0.017659\n",
            "[EPOCH: 59], Test Loss: 0.1654, Test Accuracy: 33.11 %\n",
            "Train Epoch: 60 [0/740 (0%)] Train Loss: 0.104720\n",
            "[EPOCH: 60], Test Loss: 0.1340, Test Accuracy: 35.81 %\n",
            "Train Epoch: 61 [0/740 (0%)] Train Loss: 0.019557\n",
            "[EPOCH: 61], Test Loss: 0.1466, Test Accuracy: 36.82 %\n",
            "Train Epoch: 62 [0/740 (0%)] Train Loss: 0.092959\n",
            "[EPOCH: 62], Test Loss: 0.1082, Test Accuracy: 38.51 %\n",
            "Train Epoch: 63 [0/740 (0%)] Train Loss: 0.014701\n",
            "[EPOCH: 63], Test Loss: 0.1448, Test Accuracy: 36.82 %\n",
            "Train Epoch: 64 [0/740 (0%)] Train Loss: 0.019796\n",
            "[EPOCH: 64], Test Loss: 0.1448, Test Accuracy: 31.08 %\n",
            "Train Epoch: 65 [0/740 (0%)] Train Loss: 0.013368\n",
            "[EPOCH: 65], Test Loss: 0.1211, Test Accuracy: 34.46 %\n",
            "Train Epoch: 66 [0/740 (0%)] Train Loss: 0.458742\n",
            "[EPOCH: 66], Test Loss: 0.1436, Test Accuracy: 35.47 %\n",
            "Train Epoch: 67 [0/740 (0%)] Train Loss: 0.174519\n",
            "[EPOCH: 67], Test Loss: 0.1397, Test Accuracy: 35.14 %\n",
            "Train Epoch: 68 [0/740 (0%)] Train Loss: 0.018694\n",
            "[EPOCH: 68], Test Loss: 0.1785, Test Accuracy: 31.76 %\n",
            "Train Epoch: 69 [0/740 (0%)] Train Loss: 0.100646\n",
            "[EPOCH: 69], Test Loss: 0.1017, Test Accuracy: 41.55 %\n",
            "Train Epoch: 70 [0/740 (0%)] Train Loss: 0.031601\n",
            "[EPOCH: 70], Test Loss: 0.1312, Test Accuracy: 32.77 %\n",
            "Train Epoch: 71 [0/740 (0%)] Train Loss: 0.087384\n",
            "[EPOCH: 71], Test Loss: 0.1217, Test Accuracy: 41.22 %\n",
            "Train Epoch: 72 [0/740 (0%)] Train Loss: 0.304772\n",
            "[EPOCH: 72], Test Loss: 0.1219, Test Accuracy: 33.78 %\n",
            "Train Epoch: 73 [0/740 (0%)] Train Loss: 0.128947\n",
            "[EPOCH: 73], Test Loss: 0.1417, Test Accuracy: 35.81 %\n",
            "Train Epoch: 74 [0/740 (0%)] Train Loss: 0.082103\n",
            "[EPOCH: 74], Test Loss: 0.1251, Test Accuracy: 39.86 %\n",
            "Train Epoch: 75 [0/740 (0%)] Train Loss: 0.013602\n",
            "[EPOCH: 75], Test Loss: 0.1384, Test Accuracy: 38.85 %\n",
            "Train Epoch: 76 [0/740 (0%)] Train Loss: 0.012274\n",
            "[EPOCH: 76], Test Loss: 0.1343, Test Accuracy: 34.12 %\n",
            "Train Epoch: 77 [0/740 (0%)] Train Loss: 0.009032\n",
            "[EPOCH: 77], Test Loss: 0.1624, Test Accuracy: 35.47 %\n",
            "Train Epoch: 78 [0/740 (0%)] Train Loss: 0.119390\n",
            "[EPOCH: 78], Test Loss: 0.1669, Test Accuracy: 34.46 %\n",
            "Train Epoch: 79 [0/740 (0%)] Train Loss: 0.025506\n",
            "[EPOCH: 79], Test Loss: 0.1145, Test Accuracy: 38.51 %\n",
            "Train Epoch: 80 [0/740 (0%)] Train Loss: 0.012766\n",
            "[EPOCH: 80], Test Loss: 0.1291, Test Accuracy: 38.85 %\n",
            "Train Epoch: 81 [0/740 (0%)] Train Loss: 0.069661\n",
            "[EPOCH: 81], Test Loss: 0.1651, Test Accuracy: 32.09 %\n",
            "Train Epoch: 82 [0/740 (0%)] Train Loss: 0.052681\n",
            "[EPOCH: 82], Test Loss: 0.1993, Test Accuracy: 30.07 %\n",
            "Train Epoch: 83 [0/740 (0%)] Train Loss: 0.029730\n",
            "[EPOCH: 83], Test Loss: 0.1364, Test Accuracy: 40.20 %\n",
            "Train Epoch: 84 [0/740 (0%)] Train Loss: 0.051310\n",
            "[EPOCH: 84], Test Loss: 0.1208, Test Accuracy: 37.50 %\n",
            "Train Epoch: 85 [0/740 (0%)] Train Loss: 0.005208\n",
            "[EPOCH: 85], Test Loss: 0.1821, Test Accuracy: 30.74 %\n",
            "Train Epoch: 86 [0/740 (0%)] Train Loss: 0.088443\n",
            "[EPOCH: 86], Test Loss: 0.1400, Test Accuracy: 37.84 %\n",
            "Train Epoch: 87 [0/740 (0%)] Train Loss: 0.045007\n",
            "[EPOCH: 87], Test Loss: 0.1682, Test Accuracy: 35.81 %\n",
            "Train Epoch: 88 [0/740 (0%)] Train Loss: 0.078479\n",
            "[EPOCH: 88], Test Loss: 0.1909, Test Accuracy: 30.41 %\n",
            "Train Epoch: 89 [0/740 (0%)] Train Loss: 0.045800\n",
            "[EPOCH: 89], Test Loss: 0.1940, Test Accuracy: 32.09 %\n",
            "Train Epoch: 90 [0/740 (0%)] Train Loss: 0.051219\n",
            "[EPOCH: 90], Test Loss: 0.1489, Test Accuracy: 36.49 %\n",
            "Train Epoch: 91 [0/740 (0%)] Train Loss: 0.073718\n",
            "[EPOCH: 91], Test Loss: 0.1538, Test Accuracy: 35.14 %\n",
            "Train Epoch: 92 [0/740 (0%)] Train Loss: 0.013063\n",
            "[EPOCH: 92], Test Loss: 0.1643, Test Accuracy: 35.47 %\n",
            "Train Epoch: 93 [0/740 (0%)] Train Loss: 0.040075\n",
            "[EPOCH: 93], Test Loss: 0.1436, Test Accuracy: 37.50 %\n",
            "Train Epoch: 94 [0/740 (0%)] Train Loss: 0.072717\n",
            "[EPOCH: 94], Test Loss: 0.1441, Test Accuracy: 34.46 %\n",
            "Train Epoch: 95 [0/740 (0%)] Train Loss: 0.046652\n",
            "[EPOCH: 95], Test Loss: 0.1498, Test Accuracy: 35.81 %\n",
            "Train Epoch: 96 [0/740 (0%)] Train Loss: 0.022481\n",
            "[EPOCH: 96], Test Loss: 0.1798, Test Accuracy: 34.80 %\n",
            "Train Epoch: 97 [0/740 (0%)] Train Loss: 0.051265\n",
            "[EPOCH: 97], Test Loss: 0.1845, Test Accuracy: 32.77 %\n",
            "Train Epoch: 98 [0/740 (0%)] Train Loss: 0.015723\n",
            "[EPOCH: 98], Test Loss: 0.1636, Test Accuracy: 37.84 %\n",
            "Train Epoch: 99 [0/740 (0%)] Train Loss: 0.017488\n",
            "[EPOCH: 99], Test Loss: 0.1437, Test Accuracy: 33.78 %\n",
            "Train Epoch: 100 [0/740 (0%)] Train Loss: 0.291517\n",
            "[EPOCH: 100], Test Loss: 0.1090, Test Accuracy: 39.19 %\n"
          ]
        }
      ]
    }
  ]
}
