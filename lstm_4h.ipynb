{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm-4h.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyJ/OQV6eBXnoXFxFosPem",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uquant0507/predicting-btc-price-with-lstm/blob/main/lstm_4h.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Python libraries"
      ],
      "metadata": {
        "id": "O3XQb4p3bAoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install numpy, pandas, pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "# pip install torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable \n",
        "import torch.nn.init as init\n",
        "\n",
        "# pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# pip install ccxt\n",
        "!pip install ccxt\n",
        "import ccxt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NkmbBSsbr8m",
        "outputId": "2b65039f-88cc-4f24-cbad-63a4648b17da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ccxt\n",
            "  Downloading ccxt-1.72.87-py2.py3-none-any.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 5.5 MB/s \n",
            "\u001b[?25hCollecting aiohttp>=3.8\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=38.5.1 in /usr/local/lib/python3.7/dist-packages (from ccxt) (57.4.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.7/dist-packages (from ccxt) (2021.10.8)\n",
            "Collecting cryptography>=2.6.1\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 18.8 MB/s \n",
            "\u001b[?25hCollecting aiodns>=1.1.1\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting yarl==1.7.2\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from ccxt) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt) (3.10.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt) (2.10)\n",
            "Collecting multidict>=4.0\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting pycares>=4.0.0\n",
            "  Downloading pycares-4.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt) (2.0.11)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt) (21.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.6.1->ccxt) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt) (2.21)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->ccxt) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->ccxt) (1.24.3)\n",
            "Installing collected packages: multidict, frozenlist, yarl, pycares, asynctest, async-timeout, aiosignal, cryptography, aiohttp, aiodns, ccxt\n",
            "Successfully installed aiodns-3.0.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 ccxt-1.72.87 cryptography-36.0.1 frozenlist-1.3.0 multidict-6.0.2 pycares-4.1.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"data\": {\n",
        "        \"window_size\": 14,\n",
        "        \"train_split_size\": 0.5,\n",
        "        \"val_split_size\": 0.2,\n",
        "        \"thres_frac\": 1,\n",
        "        \"change_time\": 3\n",
        "    }, \n",
        "    \"model\": {\n",
        "        \"input_size\": 2, # price, volume\n",
        "        \"num_lstm_layers\": 1,\n",
        "        \"hidden_size\": 64,\n",
        "        \"num_classes\" : 3,\n",
        "        \"dropout\": 0.2,\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"device\": \"cuda\", # \"cuda\" or \"cpu\"\n",
        "        \"batch_size\": 32,\n",
        "        \"epoch\": 100,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"scheduler_step_size\": 40,\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "tul_QAEsT60h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Preparation"
      ],
      "metadata": {
        "id": "WCfdy4fybvmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1)Fetch Data"
      ],
      "metadata": {
        "id": "u_LL3glyhyZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binance = ccxt.binance()\n",
        "fetch_num = 1000\n",
        "timeframe = '4h'\n",
        "ticker = binance.fetch_ohlcv(\"BTC/USDT\", timeframe, limit=fetch_num)\n",
        "startfrom = ticker[0][0]\n",
        "class ohlcv:\n",
        "  def __init__(self, index):\n",
        "    self.index = index\n",
        "    since = startfrom - 3600000000 * 4 * self.index\n",
        "    ohlcv = binance.fetch_ohlcv(\"BTC/USDT\", timeframe, since=since, limit=fetch_num)\n",
        "    self.df = pd.DataFrame(ohlcv, columns=['datetime', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "ohlcv_df = pd.concat([ohlcv(12).df, ohlcv(11).df, ohlcv(10).df, ohlcv(9).df, ohlcv(8).df, ohlcv(7).df, ohlcv(6).df, ohlcv(5).df, ohlcv(4).df, ohlcv(3).df, ohlcv(2).df, ohlcv(1).df, ohlcv(0).df])\n",
        "ohlcv_df = ohlcv_df.drop_duplicates(['datetime'])\n",
        "ohlcv_df = ohlcv_df.drop(ohlcv_df.loc[ohlcv_df['close']==0].index)\n",
        "ohlcv_df = ohlcv_df.drop(ohlcv_df.loc[ohlcv_df['volume']==0].index)\n",
        "ohlcv_df['p_change'] = ohlcv_df['close'] - ohlcv_df['close'].shift(1)\n",
        "ohlcv_df['p_changerate'] = np.log(ohlcv_df['close']/ ohlcv_df['close'].shift(1))\n",
        "ohlcv_df['v_changerate'] = np.log(ohlcv_df['volume']/ ohlcv_df['volume'].shift(1))\n",
        "ohlcv_df = ohlcv_df.drop([0])\n",
        "ohlcv_df['datetime'] = pd.to_datetime(ohlcv_df['datetime'], unit='ms')\n",
        "datetime = ohlcv_df['datetime'].to_numpy()\n",
        "ohlcv_df.set_index('datetime', inplace=True)\n",
        "ohlcv_df.to_excel(\"ohlcv3.xlsx\")\n",
        "print(ohlcv_df.describe())\n",
        "p_change = ohlcv_df['p_change'].to_numpy()\n",
        "p_changerate = ohlcv_df['p_changerate'].to_numpy()\n",
        "v_changerate = ohlcv_df['v_changerate'].to_numpy()\n",
        "print(p_change.shape)"
      ],
      "metadata": {
        "id": "SD2GJYpEh2Dv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a1d8df-c3c6-40c3-878b-9ac5b64ee1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               open          high  ...  p_changerate  v_changerate\n",
            "count   9827.000000   9827.000000  ...   9827.000000   9827.000000\n",
            "mean   18045.832621  18265.798387  ...      0.000237      0.000116\n",
            "std    17475.770244  17684.119669  ...      0.017781      0.484282\n",
            "min     2870.900000   3148.000000  ...     -0.229366     -6.994578\n",
            "25%     6762.985000   6841.545000  ...     -0.005773     -0.304961\n",
            "50%     9370.000000   9448.680000  ...      0.000319     -0.017231\n",
            "75%    28271.385000  28749.395000  ...      0.006598      0.280314\n",
            "max    68490.000000  69000.000000  ...      0.271624      7.488635\n",
            "\n",
            "[8 rows x 8 columns]\n",
            "(9827,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "cGiBMNxCB4pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2)Data into train, test data"
      ],
      "metadata": {
        "id": "SLIfYv0N7KF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize\n",
        "def normalize(x):\n",
        "  mu = np.average(x)\n",
        "  sd = np.std(x)\n",
        "  normalized_x = (x - mu) / sd\n",
        "  print(mu)\n",
        "  print(sd)\n",
        "  return normalized_x\n",
        "\n",
        "p_changerate = normalize(p_changerate)\n",
        "v_changerate = normalize(v_changerate)\n",
        "\n",
        "#perform windowing\n",
        "def prepare_data_x(p_changerate, v_changerate, window_size):\n",
        "   n_row = p_changerate.shape[0] - window_size + 1\n",
        "   x1 = np.lib.stride_tricks.as_strided(p_changerate, shape=(n_row,window_size), strides=(p_changerate.strides[0],p_changerate.strides[0]))\n",
        "   x2 = np.lib.stride_tricks.as_strided(v_changerate, shape=(n_row,window_size), strides=(v_changerate.strides[0],v_changerate.strides[0]))\n",
        "   li = [-i for i in range(1, config[\"data\"][\"change_time\"]+1)]\n",
        "   x1 = np.delete(x1, li, axis=0)\n",
        "   x2 = np.delete(x2, li, axis=0)\n",
        "   x = np.dstack([x1, x2])\n",
        "   x = x.astype(np.float32)\n",
        "   return x\n",
        "\n",
        "#calculate deviation by window\n",
        "def prepare_data_y(p_change, window_size, fraction):\n",
        "  n_row = p_change.shape[0] - window_size + 1\n",
        "  windowed_change = np.lib.stride_tricks.as_strided(p_change, shape=(n_row,window_size), strides=(p_change.strides[0],p_change.strides[0]))\n",
        "  threshold = fraction * np.std(windowed_change, axis=1)\n",
        "  print(threshold.shape)\n",
        "  yn = np.zeros(n_row - config[\"data\"][\"change_time\"])\n",
        "  for i in range(n_row - config[\"data\"][\"change_time\"]):\n",
        "    li = p_change[window_size+i:window_size+i+ config[\"data\"][\"change_time\"]]\n",
        "    change = sum(li)\n",
        "    if change >= threshold[i]:\n",
        "      yn[i] = 1\n",
        "    elif change <= -threshold[i]:\n",
        "      yn[i] = 2\n",
        "  return yn\n",
        "\n",
        "\n",
        "x = prepare_data_x(p_changerate, v_changerate, config[\"data\"][\"window_size\"])\n",
        "y = prepare_data_y(p_change, config[\"data\"][\"window_size\"],  config[\"data\"][\"thres_frac\"])\n",
        "\n",
        "# split dataset\n",
        "def split_dataset(x, y, shuffle=False):\n",
        "  index = int(x.shape[0]* config[\"data\"][\"train_split_size\"])\n",
        "  index2 = int(x.shape[0]* (config[\"data\"][\"train_split_size\"] + config[\"data\"][\"val_split_size\"]))\n",
        "  x_train = x[:index]\n",
        "  x_val = x[index:index2]\n",
        "  x_test = x[index2:]\n",
        "  y_train = y[:index]\n",
        "  y_val = y[index:index2]\n",
        "  y_test = y[index2:]\n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "x_train = split_dataset(x, y)[0]\n",
        "y_train = split_dataset(x, y)[1]\n",
        "x_val = split_dataset(x, y)[2]\n",
        "y_val = split_dataset(x, y)[3]\n",
        "x_test = split_dataset(x, y)[4]\n",
        "y_test = split_dataset(x, y)[5]\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x.astype(np.float32)\n",
        "        self.y = y.astype(np.int64)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.x[idx], self.y[idx])\n",
        "\n",
        "dataset_train = TimeSeriesDataset(x_train, y_train)\n",
        "dataset_val = TimeSeriesDataset(x_val, y_val)\n",
        "dataset_test = TimeSeriesDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "val_loader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "test_loader = DataLoader(dataset_test, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "\n",
        "for (x_train, y_train) in train_loader: \n",
        "  print(x_train.size(), x_train.type())\n",
        "  print(y_train.size(), y_train.type())\n",
        "  break"
      ],
      "metadata": {
        "id": "NGNAHCnv7Xxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64606124-9040-44bb-cddc-4581d3cdcb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00023652873749213717\n",
            "0.017780092687123123\n",
            "0.00011561125948424708\n",
            "0.48425696334908436\n",
            "(9814,)\n",
            "torch.Size([32, 14, 2]) torch.FloatTensor\n",
            "torch.Size([32]) torch.LongTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Defining Model"
      ],
      "metadata": {
        "id": "vyFETWkdIkeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes #number of classes\n",
        "    self.num_layers = num_layers #number of layers\n",
        "    self.input_size = input_size #input size\n",
        "    self.hidden_size = hidden_size #hidden state\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm1\n",
        "    self.fc1 = nn.Linear(hidden_size, 512) #fully connected 1\n",
        "    self.fc2 = nn.Linear(512, 128)\n",
        "    self.fc3 = nn.Linear(128, num_classes) #fully connected last layer\n",
        "    self.relu = nn.ReLU()\n",
        "    self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(512)\n",
        "    self.batch_norm3 = nn.BatchNorm1d(128)\n",
        "    self.dropout_prob = 0.2\n",
        "  \n",
        "  def forward(self,x):\n",
        "    # Propagate input through LSTM\n",
        "    output, (hn, cn) = self.lstm(x) #lstm with input, hidden, and internal state\n",
        "    x = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "    x = self.batch_norm1(x)\n",
        "    x = self.fc1(x) #first Dense\n",
        "    x = self.batch_norm2(x)\n",
        "    x = self.relu(x) #relu\n",
        "    x = F.dropout(x, training=self.training, p=self.dropout_prob)\n",
        "    x = self.fc2(x) #first Dense\n",
        "    x = self.batch_norm3(x)\n",
        "    x = self.relu(x) #relu\n",
        "    x = F.dropout(x, training=self.training, p=self.dropout_prob)\n",
        "    x = self.fc3(x) #Final Output\n",
        "    x = F.log_softmax(x)\n",
        "    return x\n",
        "\n",
        "model = LSTMModel(num_classes=config[\"model\"][\"num_classes\"], input_size=config[\"model\"][\"input_size\"], hidden_size=config[\"model\"][\"hidden_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"])\n",
        "model = model.to(config[\"training\"][\"device\"])"
      ],
      "metadata": {
        "id": "G7aAqzloInl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Optimizer, Objective Function 설정하기"
      ],
      "metadata": {
        "id": "4qnz2nMUpB5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def weight_init(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    nn.init.kaiming_uniform(m.weight.data)\n",
        "model.apply(weight_init)"
      ],
      "metadata": {
        "id": "bK14LBmApNgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c10bcf-d7e0-4c91-c87a-450c8df9a0f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(2, 32, batch_first=True)\n",
              "  (fc1): Linear(in_features=32, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (batch_norm1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Train model"
      ],
      "metadata": {
        "id": "i38xX6mdpk9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, log_interval = 20):\n",
        "  model.train()\n",
        "  for idx, (x, y) in enumerate(train_loader):\n",
        "    x = x.to(config[\"training\"][\"device\"])\n",
        "    y = y.to(config[\"training\"][\"device\"])\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if idx % log_interval == 0:\n",
        "      print(\"Train Epoch: {} [{}/{} ({:.0f}%)] Train Loss: {:.6f}\".format(epoch, idx * len(x),\n",
        "      len(train_loader.dataset), 100. * idx / len(train_loader), loss.item()))\n",
        "    if idx == len(train_loader) - 1:\n",
        "      return loss.item()"
      ],
      "metadata": {
        "id": "F2aO4Muapnls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Evaluate Model"
      ],
      "metadata": {
        "id": "b_-0qRXWRzHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  ups = 0 \n",
        "  downs = 0\n",
        "  zeros = 0 \n",
        "  up_true_positive = 0\n",
        "  down_true_positive = 0\n",
        "  zero_true_positive = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "      x = x.to(config[\"training\"][\"device\"])\n",
        "      y = y.to(config[\"training\"][\"device\"])\n",
        "      out = model(x)\n",
        "      test_loss += criterion(out, y).item()\n",
        "      prediction = out.max(1, True)[1]\n",
        "      label = y.view_as(prediction)\n",
        "      correct += prediction.eq(label).sum().item()\n",
        "      ups += (prediction == 1).sum().item()\n",
        "      downs += (prediction == 2).sum().item()\n",
        "      zeros += (prediction == 0).sum().item()\n",
        "      for i in range(len(prediction)):\n",
        "        if prediction[i] == 1:\n",
        "          if prediction[i] == label[i]:\n",
        "            up_true_positive += 1\n",
        "        elif prediction[i] == 2:\n",
        "          if prediction[i] == label[i]:\n",
        "            down_true_positive += 1       \n",
        "        else: \n",
        "          if prediction[i] == label[i]:\n",
        "            zero_true_positive += 1 \n",
        "\n",
        "  up_accuracy = 100 * up_true_positive / (ups + 1e-7)\n",
        "  down_accuracy = 100 * down_true_positive / (downs + 1e-7)\n",
        "  zero_accuracy = 100 * zero_true_positive / (zeros + 1e-7)\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "  return test_loss, test_accuracy, prediction, out, up_accuracy, down_accuracy, zero_accuracy"
      ],
      "metadata": {
        "id": "aGDJDH-bR3V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 학습 진행 및 평가"
      ],
      "metadata": {
        "id": "LEC1sTssUBHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloss = []\n",
        "valloss = []\n",
        "valaccuracy = []\n",
        "upaccuracy = []\n",
        "downaccuracy = []\n",
        "zeroaccuracy = []\n",
        "for epoch in range(1, config[\"training\"][\"epoch\"] + 1):\n",
        "  x = train(model, train_loader, optimizer, log_interval=100)\n",
        "  trainloss.append(x)\n",
        "  test_loss, test_accuracy, prediction, out, up_accuracy, down_accuracy, zero_accuracy = evaluate(model, val_loader)\n",
        "  valloss.append(test_loss)\n",
        "  valaccuracy.append(test_accuracy)\n",
        "  upaccuracy.append(up_accuracy)\n",
        "  downaccuracy.append(down_accuracy)\n",
        "  zeroaccuracy.append(zero_accuracy)\n",
        "  print(\"[EPOCH: {}], Test Loss: {:.4f}, Test Accuracy: {:.2f} %\".format(epoch, test_loss, test_accuracy))\n",
        "val_data = {'train loss' : trainloss, 'val loss' : valloss, 'val accuracy' : valaccuracy, 'up accuracy' : upaccuracy, 'down accuracy' : downaccuracy, 'zero accuracy' : zeroaccuracy}\n",
        "val_result = pd.DataFrame(val_data)\n",
        "val_result.to_excel(\"val_result.xlsx\")\n"
      ],
      "metadata": {
        "id": "sTpRqo8dUiDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8efb278-bf48-4cdf-c8fb-536b805d8c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/4905 (0%)] Train Loss: 1.517689\n",
            "Train Epoch: 1 [3200/4905 (65%)] Train Loss: 0.917571\n",
            "[EPOCH: 1], Test Loss: 0.0398, Test Accuracy: 49.13 %\n",
            "Train Epoch: 2 [0/4905 (0%)] Train Loss: 1.314785\n",
            "Train Epoch: 2 [3200/4905 (65%)] Train Loss: 0.819714\n",
            "[EPOCH: 2], Test Loss: 0.0362, Test Accuracy: 52.55 %\n",
            "Train Epoch: 3 [0/4905 (0%)] Train Loss: 1.210282\n",
            "Train Epoch: 3 [3200/4905 (65%)] Train Loss: 0.778541\n",
            "[EPOCH: 3], Test Loss: 0.0332, Test Accuracy: 52.09 %\n",
            "Train Epoch: 4 [0/4905 (0%)] Train Loss: 1.206837\n",
            "Train Epoch: 4 [3200/4905 (65%)] Train Loss: 0.866792\n",
            "[EPOCH: 4], Test Loss: 0.0377, Test Accuracy: 51.27 %\n",
            "Train Epoch: 5 [0/4905 (0%)] Train Loss: 1.116573\n",
            "Train Epoch: 5 [3200/4905 (65%)] Train Loss: 0.772534\n",
            "[EPOCH: 5], Test Loss: 0.0412, Test Accuracy: 44.85 %\n",
            "Train Epoch: 6 [0/4905 (0%)] Train Loss: 1.147320\n",
            "Train Epoch: 6 [3200/4905 (65%)] Train Loss: 0.736505\n",
            "[EPOCH: 6], Test Loss: 0.0384, Test Accuracy: 47.66 %\n",
            "Train Epoch: 7 [0/4905 (0%)] Train Loss: 1.192726\n",
            "Train Epoch: 7 [3200/4905 (65%)] Train Loss: 0.744192\n",
            "[EPOCH: 7], Test Loss: 0.0391, Test Accuracy: 48.67 %\n",
            "Train Epoch: 8 [0/4905 (0%)] Train Loss: 1.069429\n",
            "Train Epoch: 8 [3200/4905 (65%)] Train Loss: 0.733790\n",
            "[EPOCH: 8], Test Loss: 0.0509, Test Accuracy: 46.84 %\n",
            "Train Epoch: 9 [0/4905 (0%)] Train Loss: 1.292470\n",
            "Train Epoch: 9 [3200/4905 (65%)] Train Loss: 0.683146\n",
            "[EPOCH: 9], Test Loss: 0.0489, Test Accuracy: 46.64 %\n",
            "Train Epoch: 10 [0/4905 (0%)] Train Loss: 1.241285\n",
            "Train Epoch: 10 [3200/4905 (65%)] Train Loss: 0.788329\n",
            "[EPOCH: 10], Test Loss: 0.0439, Test Accuracy: 50.10 %\n",
            "Train Epoch: 11 [0/4905 (0%)] Train Loss: 0.980576\n",
            "Train Epoch: 11 [3200/4905 (65%)] Train Loss: 0.726154\n",
            "[EPOCH: 11], Test Loss: 0.0433, Test Accuracy: 48.11 %\n",
            "Train Epoch: 12 [0/4905 (0%)] Train Loss: 1.049856\n",
            "Train Epoch: 12 [3200/4905 (65%)] Train Loss: 0.713310\n",
            "[EPOCH: 12], Test Loss: 0.0446, Test Accuracy: 45.92 %\n",
            "Train Epoch: 13 [0/4905 (0%)] Train Loss: 0.953786\n",
            "Train Epoch: 13 [3200/4905 (65%)] Train Loss: 0.703708\n",
            "[EPOCH: 13], Test Loss: 0.0412, Test Accuracy: 47.96 %\n",
            "Train Epoch: 14 [0/4905 (0%)] Train Loss: 0.874009\n",
            "Train Epoch: 14 [3200/4905 (65%)] Train Loss: 0.660579\n",
            "[EPOCH: 14], Test Loss: 0.0431, Test Accuracy: 49.13 %\n",
            "Train Epoch: 15 [0/4905 (0%)] Train Loss: 0.847590\n",
            "Train Epoch: 15 [3200/4905 (65%)] Train Loss: 0.556016\n",
            "[EPOCH: 15], Test Loss: 0.0470, Test Accuracy: 48.78 %\n",
            "Train Epoch: 16 [0/4905 (0%)] Train Loss: 0.788008\n",
            "Train Epoch: 16 [3200/4905 (65%)] Train Loss: 0.588954\n",
            "[EPOCH: 16], Test Loss: 0.0431, Test Accuracy: 47.81 %\n",
            "Train Epoch: 17 [0/4905 (0%)] Train Loss: 0.934822\n",
            "Train Epoch: 17 [3200/4905 (65%)] Train Loss: 0.545973\n",
            "[EPOCH: 17], Test Loss: 0.0550, Test Accuracy: 45.26 %\n",
            "Train Epoch: 18 [0/4905 (0%)] Train Loss: 0.770549\n",
            "Train Epoch: 18 [3200/4905 (65%)] Train Loss: 0.612965\n",
            "[EPOCH: 18], Test Loss: 0.0507, Test Accuracy: 48.37 %\n",
            "Train Epoch: 19 [0/4905 (0%)] Train Loss: 0.998695\n",
            "Train Epoch: 19 [3200/4905 (65%)] Train Loss: 0.688636\n",
            "[EPOCH: 19], Test Loss: 0.0533, Test Accuracy: 47.96 %\n",
            "Train Epoch: 20 [0/4905 (0%)] Train Loss: 0.779868\n",
            "Train Epoch: 20 [3200/4905 (65%)] Train Loss: 0.595452\n",
            "[EPOCH: 20], Test Loss: 0.0558, Test Accuracy: 47.91 %\n",
            "Train Epoch: 21 [0/4905 (0%)] Train Loss: 0.807697\n",
            "Train Epoch: 21 [3200/4905 (65%)] Train Loss: 0.545672\n",
            "[EPOCH: 21], Test Loss: 0.0559, Test Accuracy: 47.81 %\n",
            "Train Epoch: 22 [0/4905 (0%)] Train Loss: 0.801075\n",
            "Train Epoch: 22 [3200/4905 (65%)] Train Loss: 0.588352\n",
            "[EPOCH: 22], Test Loss: 0.0564, Test Accuracy: 48.06 %\n",
            "Train Epoch: 23 [0/4905 (0%)] Train Loss: 0.785191\n",
            "Train Epoch: 23 [3200/4905 (65%)] Train Loss: 0.603795\n",
            "[EPOCH: 23], Test Loss: 0.0660, Test Accuracy: 47.40 %\n",
            "Train Epoch: 24 [0/4905 (0%)] Train Loss: 0.695500\n",
            "Train Epoch: 24 [3200/4905 (65%)] Train Loss: 0.449746\n",
            "[EPOCH: 24], Test Loss: 0.0615, Test Accuracy: 46.23 %\n",
            "Train Epoch: 25 [0/4905 (0%)] Train Loss: 0.749475\n",
            "Train Epoch: 25 [3200/4905 (65%)] Train Loss: 0.489152\n",
            "[EPOCH: 25], Test Loss: 0.0664, Test Accuracy: 45.16 %\n",
            "Train Epoch: 26 [0/4905 (0%)] Train Loss: 0.757701\n",
            "Train Epoch: 26 [3200/4905 (65%)] Train Loss: 0.395717\n",
            "[EPOCH: 26], Test Loss: 0.0767, Test Accuracy: 45.01 %\n",
            "Train Epoch: 27 [0/4905 (0%)] Train Loss: 0.443489\n",
            "Train Epoch: 27 [3200/4905 (65%)] Train Loss: 0.547835\n",
            "[EPOCH: 27], Test Loss: 0.0736, Test Accuracy: 45.21 %\n",
            "Train Epoch: 28 [0/4905 (0%)] Train Loss: 0.620013\n",
            "Train Epoch: 28 [3200/4905 (65%)] Train Loss: 0.446855\n",
            "[EPOCH: 28], Test Loss: 0.0712, Test Accuracy: 45.67 %\n",
            "Train Epoch: 29 [0/4905 (0%)] Train Loss: 0.698899\n",
            "Train Epoch: 29 [3200/4905 (65%)] Train Loss: 0.390049\n",
            "[EPOCH: 29], Test Loss: 0.0757, Test Accuracy: 43.68 %\n",
            "Train Epoch: 30 [0/4905 (0%)] Train Loss: 0.541307\n",
            "Train Epoch: 30 [3200/4905 (65%)] Train Loss: 0.389094\n",
            "[EPOCH: 30], Test Loss: 0.0674, Test Accuracy: 44.44 %\n",
            "Train Epoch: 31 [0/4905 (0%)] Train Loss: 0.503388\n",
            "Train Epoch: 31 [3200/4905 (65%)] Train Loss: 0.342309\n",
            "[EPOCH: 31], Test Loss: 0.0741, Test Accuracy: 45.41 %\n",
            "Train Epoch: 32 [0/4905 (0%)] Train Loss: 0.462196\n",
            "Train Epoch: 32 [3200/4905 (65%)] Train Loss: 0.325567\n",
            "[EPOCH: 32], Test Loss: 0.0817, Test Accuracy: 45.06 %\n",
            "Train Epoch: 33 [0/4905 (0%)] Train Loss: 0.441874\n",
            "Train Epoch: 33 [3200/4905 (65%)] Train Loss: 0.357702\n",
            "[EPOCH: 33], Test Loss: 0.0780, Test Accuracy: 45.16 %\n",
            "Train Epoch: 34 [0/4905 (0%)] Train Loss: 0.412489\n",
            "Train Epoch: 34 [3200/4905 (65%)] Train Loss: 0.336474\n",
            "[EPOCH: 34], Test Loss: 0.0795, Test Accuracy: 44.60 %\n",
            "Train Epoch: 35 [0/4905 (0%)] Train Loss: 0.686449\n",
            "Train Epoch: 35 [3200/4905 (65%)] Train Loss: 0.563792\n",
            "[EPOCH: 35], Test Loss: 0.0758, Test Accuracy: 44.95 %\n",
            "Train Epoch: 36 [0/4905 (0%)] Train Loss: 0.559629\n",
            "Train Epoch: 36 [3200/4905 (65%)] Train Loss: 0.316245\n",
            "[EPOCH: 36], Test Loss: 0.0827, Test Accuracy: 45.72 %\n",
            "Train Epoch: 37 [0/4905 (0%)] Train Loss: 0.391462\n",
            "Train Epoch: 37 [3200/4905 (65%)] Train Loss: 0.330284\n",
            "[EPOCH: 37], Test Loss: 0.0735, Test Accuracy: 44.85 %\n",
            "Train Epoch: 38 [0/4905 (0%)] Train Loss: 0.376322\n",
            "Train Epoch: 38 [3200/4905 (65%)] Train Loss: 0.213030\n",
            "[EPOCH: 38], Test Loss: 0.0806, Test Accuracy: 46.28 %\n",
            "Train Epoch: 39 [0/4905 (0%)] Train Loss: 0.212270\n",
            "Train Epoch: 39 [3200/4905 (65%)] Train Loss: 0.251170\n",
            "[EPOCH: 39], Test Loss: 0.0882, Test Accuracy: 46.84 %\n",
            "Train Epoch: 40 [0/4905 (0%)] Train Loss: 0.322429\n",
            "Train Epoch: 40 [3200/4905 (65%)] Train Loss: 0.315104\n",
            "[EPOCH: 40], Test Loss: 0.0904, Test Accuracy: 46.74 %\n",
            "Train Epoch: 41 [0/4905 (0%)] Train Loss: 0.252895\n",
            "Train Epoch: 41 [3200/4905 (65%)] Train Loss: 0.206604\n",
            "[EPOCH: 41], Test Loss: 0.0912, Test Accuracy: 46.74 %\n",
            "Train Epoch: 42 [0/4905 (0%)] Train Loss: 0.322543\n",
            "Train Epoch: 42 [3200/4905 (65%)] Train Loss: 0.314790\n",
            "[EPOCH: 42], Test Loss: 0.0940, Test Accuracy: 44.90 %\n",
            "Train Epoch: 43 [0/4905 (0%)] Train Loss: 0.366209\n",
            "Train Epoch: 43 [3200/4905 (65%)] Train Loss: 0.224941\n",
            "[EPOCH: 43], Test Loss: 0.0919, Test Accuracy: 47.25 %\n",
            "Train Epoch: 44 [0/4905 (0%)] Train Loss: 0.377021\n",
            "Train Epoch: 44 [3200/4905 (65%)] Train Loss: 0.348110\n",
            "[EPOCH: 44], Test Loss: 0.0871, Test Accuracy: 47.40 %\n",
            "Train Epoch: 45 [0/4905 (0%)] Train Loss: 0.288437\n",
            "Train Epoch: 45 [3200/4905 (65%)] Train Loss: 0.189269\n",
            "[EPOCH: 45], Test Loss: 0.0956, Test Accuracy: 46.18 %\n",
            "Train Epoch: 46 [0/4905 (0%)] Train Loss: 0.261148\n",
            "Train Epoch: 46 [3200/4905 (65%)] Train Loss: 0.176876\n",
            "[EPOCH: 46], Test Loss: 0.1105, Test Accuracy: 45.06 %\n",
            "Train Epoch: 47 [0/4905 (0%)] Train Loss: 0.544512\n",
            "Train Epoch: 47 [3200/4905 (65%)] Train Loss: 0.211944\n",
            "[EPOCH: 47], Test Loss: 0.1005, Test Accuracy: 42.81 %\n",
            "Train Epoch: 48 [0/4905 (0%)] Train Loss: 0.613539\n",
            "Train Epoch: 48 [3200/4905 (65%)] Train Loss: 0.371398\n",
            "[EPOCH: 48], Test Loss: 0.0994, Test Accuracy: 46.08 %\n",
            "Train Epoch: 49 [0/4905 (0%)] Train Loss: 0.230195\n",
            "Train Epoch: 49 [3200/4905 (65%)] Train Loss: 0.157560\n",
            "[EPOCH: 49], Test Loss: 0.1149, Test Accuracy: 45.87 %\n",
            "Train Epoch: 50 [0/4905 (0%)] Train Loss: 0.435692\n",
            "Train Epoch: 50 [3200/4905 (65%)] Train Loss: 0.183398\n",
            "[EPOCH: 50], Test Loss: 0.1174, Test Accuracy: 46.02 %\n",
            "Train Epoch: 51 [0/4905 (0%)] Train Loss: 0.300458\n",
            "Train Epoch: 51 [3200/4905 (65%)] Train Loss: 0.268413\n",
            "[EPOCH: 51], Test Loss: 0.1073, Test Accuracy: 46.69 %\n",
            "Train Epoch: 52 [0/4905 (0%)] Train Loss: 0.177231\n",
            "Train Epoch: 52 [3200/4905 (65%)] Train Loss: 0.270313\n",
            "[EPOCH: 52], Test Loss: 0.1056, Test Accuracy: 45.36 %\n",
            "Train Epoch: 53 [0/4905 (0%)] Train Loss: 0.205965\n",
            "Train Epoch: 53 [3200/4905 (65%)] Train Loss: 0.382632\n",
            "[EPOCH: 53], Test Loss: 0.1195, Test Accuracy: 46.02 %\n",
            "Train Epoch: 54 [0/4905 (0%)] Train Loss: 0.206063\n",
            "Train Epoch: 54 [3200/4905 (65%)] Train Loss: 0.243256\n",
            "[EPOCH: 54], Test Loss: 0.1124, Test Accuracy: 43.93 %\n",
            "Train Epoch: 55 [0/4905 (0%)] Train Loss: 0.185005\n",
            "Train Epoch: 55 [3200/4905 (65%)] Train Loss: 0.191775\n",
            "[EPOCH: 55], Test Loss: 0.1501, Test Accuracy: 46.13 %\n",
            "Train Epoch: 56 [0/4905 (0%)] Train Loss: 0.359253\n",
            "Train Epoch: 56 [3200/4905 (65%)] Train Loss: 0.251660\n",
            "[EPOCH: 56], Test Loss: 0.1323, Test Accuracy: 45.01 %\n",
            "Train Epoch: 57 [0/4905 (0%)] Train Loss: 0.221844\n",
            "Train Epoch: 57 [3200/4905 (65%)] Train Loss: 0.129856\n",
            "[EPOCH: 57], Test Loss: 0.1263, Test Accuracy: 45.62 %\n",
            "Train Epoch: 58 [0/4905 (0%)] Train Loss: 0.150252\n",
            "Train Epoch: 58 [3200/4905 (65%)] Train Loss: 0.151109\n",
            "[EPOCH: 58], Test Loss: 0.1315, Test Accuracy: 45.26 %\n",
            "Train Epoch: 59 [0/4905 (0%)] Train Loss: 0.221143\n",
            "Train Epoch: 59 [3200/4905 (65%)] Train Loss: 0.170396\n",
            "[EPOCH: 59], Test Loss: 0.1373, Test Accuracy: 44.75 %\n",
            "Train Epoch: 60 [0/4905 (0%)] Train Loss: 0.184600\n",
            "Train Epoch: 60 [3200/4905 (65%)] Train Loss: 0.165454\n",
            "[EPOCH: 60], Test Loss: 0.1151, Test Accuracy: 46.13 %\n",
            "Train Epoch: 61 [0/4905 (0%)] Train Loss: 0.119195\n",
            "Train Epoch: 61 [3200/4905 (65%)] Train Loss: 0.138052\n",
            "[EPOCH: 61], Test Loss: 0.1197, Test Accuracy: 44.70 %\n",
            "Train Epoch: 62 [0/4905 (0%)] Train Loss: 0.333384\n",
            "Train Epoch: 62 [3200/4905 (65%)] Train Loss: 0.145269\n",
            "[EPOCH: 62], Test Loss: 0.1113, Test Accuracy: 45.62 %\n",
            "Train Epoch: 63 [0/4905 (0%)] Train Loss: 0.334519\n",
            "Train Epoch: 63 [3200/4905 (65%)] Train Loss: 0.193576\n",
            "[EPOCH: 63], Test Loss: 0.1253, Test Accuracy: 46.64 %\n",
            "Train Epoch: 64 [0/4905 (0%)] Train Loss: 0.180304\n",
            "Train Epoch: 64 [3200/4905 (65%)] Train Loss: 0.122845\n",
            "[EPOCH: 64], Test Loss: 0.1334, Test Accuracy: 45.11 %\n",
            "Train Epoch: 65 [0/4905 (0%)] Train Loss: 0.121976\n",
            "Train Epoch: 65 [3200/4905 (65%)] Train Loss: 0.214658\n",
            "[EPOCH: 65], Test Loss: 0.1345, Test Accuracy: 45.41 %\n",
            "Train Epoch: 66 [0/4905 (0%)] Train Loss: 0.160114\n",
            "Train Epoch: 66 [3200/4905 (65%)] Train Loss: 0.064132\n",
            "[EPOCH: 66], Test Loss: 0.1396, Test Accuracy: 47.09 %\n",
            "Train Epoch: 67 [0/4905 (0%)] Train Loss: 0.213665\n",
            "Train Epoch: 67 [3200/4905 (65%)] Train Loss: 0.075920\n",
            "[EPOCH: 67], Test Loss: 0.1326, Test Accuracy: 44.19 %\n",
            "Train Epoch: 68 [0/4905 (0%)] Train Loss: 0.134064\n",
            "Train Epoch: 68 [3200/4905 (65%)] Train Loss: 0.095461\n",
            "[EPOCH: 68], Test Loss: 0.1437, Test Accuracy: 45.11 %\n",
            "Train Epoch: 69 [0/4905 (0%)] Train Loss: 0.223511\n",
            "Train Epoch: 69 [3200/4905 (65%)] Train Loss: 0.322507\n",
            "[EPOCH: 69], Test Loss: 0.1403, Test Accuracy: 46.33 %\n",
            "Train Epoch: 70 [0/4905 (0%)] Train Loss: 0.263134\n",
            "Train Epoch: 70 [3200/4905 (65%)] Train Loss: 0.236028\n",
            "[EPOCH: 70], Test Loss: 0.1349, Test Accuracy: 46.08 %\n",
            "Train Epoch: 71 [0/4905 (0%)] Train Loss: 0.150860\n",
            "Train Epoch: 71 [3200/4905 (65%)] Train Loss: 0.131694\n",
            "[EPOCH: 71], Test Loss: 0.1465, Test Accuracy: 46.48 %\n",
            "Train Epoch: 72 [0/4905 (0%)] Train Loss: 0.251227\n",
            "Train Epoch: 72 [3200/4905 (65%)] Train Loss: 0.153611\n",
            "[EPOCH: 72], Test Loss: 0.1565, Test Accuracy: 47.40 %\n",
            "Train Epoch: 73 [0/4905 (0%)] Train Loss: 0.263657\n",
            "Train Epoch: 73 [3200/4905 (65%)] Train Loss: 0.144134\n",
            "[EPOCH: 73], Test Loss: 0.1402, Test Accuracy: 46.79 %\n",
            "Train Epoch: 74 [0/4905 (0%)] Train Loss: 0.062711\n",
            "Train Epoch: 74 [3200/4905 (65%)] Train Loss: 0.090223\n",
            "[EPOCH: 74], Test Loss: 0.1715, Test Accuracy: 45.77 %\n",
            "Train Epoch: 75 [0/4905 (0%)] Train Loss: 0.112762\n",
            "Train Epoch: 75 [3200/4905 (65%)] Train Loss: 0.104744\n",
            "[EPOCH: 75], Test Loss: 0.1469, Test Accuracy: 46.94 %\n",
            "Train Epoch: 76 [0/4905 (0%)] Train Loss: 0.129396\n",
            "Train Epoch: 76 [3200/4905 (65%)] Train Loss: 0.059791\n",
            "[EPOCH: 76], Test Loss: 0.1455, Test Accuracy: 45.97 %\n",
            "Train Epoch: 77 [0/4905 (0%)] Train Loss: 0.347663\n",
            "Train Epoch: 77 [3200/4905 (65%)] Train Loss: 0.172578\n",
            "[EPOCH: 77], Test Loss: 0.1587, Test Accuracy: 47.20 %\n",
            "Train Epoch: 78 [0/4905 (0%)] Train Loss: 0.202257\n",
            "Train Epoch: 78 [3200/4905 (65%)] Train Loss: 0.156269\n",
            "[EPOCH: 78], Test Loss: 0.1539, Test Accuracy: 45.87 %\n",
            "Train Epoch: 79 [0/4905 (0%)] Train Loss: 0.072010\n",
            "Train Epoch: 79 [3200/4905 (65%)] Train Loss: 0.085719\n",
            "[EPOCH: 79], Test Loss: 0.1341, Test Accuracy: 46.89 %\n",
            "Train Epoch: 80 [0/4905 (0%)] Train Loss: 0.092624\n",
            "Train Epoch: 80 [3200/4905 (65%)] Train Loss: 0.413309\n",
            "[EPOCH: 80], Test Loss: 0.1473, Test Accuracy: 46.53 %\n",
            "Train Epoch: 81 [0/4905 (0%)] Train Loss: 0.191389\n",
            "Train Epoch: 81 [3200/4905 (65%)] Train Loss: 0.051349\n",
            "[EPOCH: 81], Test Loss: 0.1594, Test Accuracy: 47.25 %\n",
            "Train Epoch: 82 [0/4905 (0%)] Train Loss: 0.164729\n",
            "Train Epoch: 82 [3200/4905 (65%)] Train Loss: 0.307732\n",
            "[EPOCH: 82], Test Loss: 0.1638, Test Accuracy: 46.08 %\n",
            "Train Epoch: 83 [0/4905 (0%)] Train Loss: 0.233139\n",
            "Train Epoch: 83 [3200/4905 (65%)] Train Loss: 0.082566\n",
            "[EPOCH: 83], Test Loss: 0.1504, Test Accuracy: 46.08 %\n",
            "Train Epoch: 84 [0/4905 (0%)] Train Loss: 0.319524\n",
            "Train Epoch: 84 [3200/4905 (65%)] Train Loss: 0.111622\n",
            "[EPOCH: 84], Test Loss: 0.1417, Test Accuracy: 47.20 %\n",
            "Train Epoch: 85 [0/4905 (0%)] Train Loss: 0.093314\n",
            "Train Epoch: 85 [3200/4905 (65%)] Train Loss: 0.153512\n",
            "[EPOCH: 85], Test Loss: 0.1708, Test Accuracy: 44.70 %\n",
            "Train Epoch: 86 [0/4905 (0%)] Train Loss: 0.155797\n",
            "Train Epoch: 86 [3200/4905 (65%)] Train Loss: 0.200236\n",
            "[EPOCH: 86], Test Loss: 0.1534, Test Accuracy: 46.89 %\n",
            "Train Epoch: 87 [0/4905 (0%)] Train Loss: 0.294085\n",
            "Train Epoch: 87 [3200/4905 (65%)] Train Loss: 0.076603\n",
            "[EPOCH: 87], Test Loss: 0.1566, Test Accuracy: 45.16 %\n",
            "Train Epoch: 88 [0/4905 (0%)] Train Loss: 0.122267\n",
            "Train Epoch: 88 [3200/4905 (65%)] Train Loss: 0.155280\n",
            "[EPOCH: 88], Test Loss: 0.1806, Test Accuracy: 46.38 %\n",
            "Train Epoch: 89 [0/4905 (0%)] Train Loss: 0.125898\n",
            "Train Epoch: 89 [3200/4905 (65%)] Train Loss: 0.071336\n",
            "[EPOCH: 89], Test Loss: 0.1653, Test Accuracy: 46.43 %\n",
            "Train Epoch: 90 [0/4905 (0%)] Train Loss: 0.168664\n",
            "Train Epoch: 90 [3200/4905 (65%)] Train Loss: 0.069286\n",
            "[EPOCH: 90], Test Loss: 0.1683, Test Accuracy: 46.59 %\n",
            "Train Epoch: 91 [0/4905 (0%)] Train Loss: 0.297742\n",
            "Train Epoch: 91 [3200/4905 (65%)] Train Loss: 0.119921\n",
            "[EPOCH: 91], Test Loss: 0.1531, Test Accuracy: 46.13 %\n",
            "Train Epoch: 92 [0/4905 (0%)] Train Loss: 0.334190\n",
            "Train Epoch: 92 [3200/4905 (65%)] Train Loss: 0.294179\n",
            "[EPOCH: 92], Test Loss: 0.1673, Test Accuracy: 45.72 %\n",
            "Train Epoch: 93 [0/4905 (0%)] Train Loss: 0.197922\n",
            "Train Epoch: 93 [3200/4905 (65%)] Train Loss: 0.125195\n",
            "[EPOCH: 93], Test Loss: 0.1632, Test Accuracy: 46.59 %\n",
            "Train Epoch: 94 [0/4905 (0%)] Train Loss: 0.113724\n",
            "Train Epoch: 94 [3200/4905 (65%)] Train Loss: 0.076946\n",
            "[EPOCH: 94], Test Loss: 0.1657, Test Accuracy: 46.08 %\n",
            "Train Epoch: 95 [0/4905 (0%)] Train Loss: 0.203069\n",
            "Train Epoch: 95 [3200/4905 (65%)] Train Loss: 0.133322\n",
            "[EPOCH: 95], Test Loss: 0.1623, Test Accuracy: 47.55 %\n",
            "Train Epoch: 96 [0/4905 (0%)] Train Loss: 0.148536\n",
            "Train Epoch: 96 [3200/4905 (65%)] Train Loss: 0.068846\n",
            "[EPOCH: 96], Test Loss: 0.1750, Test Accuracy: 48.01 %\n",
            "Train Epoch: 97 [0/4905 (0%)] Train Loss: 0.110335\n",
            "Train Epoch: 97 [3200/4905 (65%)] Train Loss: 0.153325\n",
            "[EPOCH: 97], Test Loss: 0.1728, Test Accuracy: 46.79 %\n",
            "Train Epoch: 98 [0/4905 (0%)] Train Loss: 0.270593\n",
            "Train Epoch: 98 [3200/4905 (65%)] Train Loss: 0.202862\n",
            "[EPOCH: 98], Test Loss: 0.1625, Test Accuracy: 48.06 %\n",
            "Train Epoch: 99 [0/4905 (0%)] Train Loss: 0.096679\n",
            "Train Epoch: 99 [3200/4905 (65%)] Train Loss: 0.185292\n",
            "[EPOCH: 99], Test Loss: 0.1813, Test Accuracy: 46.02 %\n",
            "Train Epoch: 100 [0/4905 (0%)] Train Loss: 0.118056\n",
            "Train Epoch: 100 [3200/4905 (65%)] Train Loss: 0.042852\n",
            "[EPOCH: 100], Test Loss: 0.2159, Test Accuracy: 45.21 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((y_val==0).sum())\n",
        "print((y_val==1).sum())\n",
        "print((y_val==2).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q0ihLqtOz3c",
        "outputId": "002c7a80-9d17-4eb1-8016-645cac78cdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1031\n",
            "496\n",
            "435\n"
          ]
        }
      ]
    }
  ]
}